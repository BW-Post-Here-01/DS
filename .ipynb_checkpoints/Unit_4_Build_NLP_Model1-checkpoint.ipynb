{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is a model for NLP that allows us to clean data and correctly categorize each post from reddit as it was classified.  Step by step the model will:\n",
    "1) clean and process data\n",
    "2) vectorize data\n",
    "3) fit a model with spacy\n",
    "4) score the model with a confusion matrix\n",
    "5) test the model\n",
    "6) pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data, mined by Jonathan\n",
    "url = 'https://raw.githubusercontent.com/BW-Post-Here-01/DS/master/Data/reddit_data_slimmed.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>COMMUNITY ANNOUNCEMENT In solidarity with the ...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Weekly r/Tattoos Question/FreeTalk Thread! - A...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Enter Shikari and Architects album artwork and...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>David Bowie Portrait - Healed, Done in April 2...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Photo realism artist chicago As the title sugg...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Can you do colorful Japanese/Yakuza tattoos on...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Tattoo Commission Question. Seperate Artist an...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Weekly r/Tattoos Question/FreeTalk Thread! - A...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Partial cover up / adding to a design with a d...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Lately I realized, that very famous people, ha...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Sacramento area artists? Hello, so after the q...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>ISO Canadian Fish / Wildlife Artist around the...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>As a client, what should I provide my artist w...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Sleeve thoughts Hello fellow redditors! I have...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Weekly r/Tattoos Question/FreeTalk Thread! - A...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Healed tattoo I got about 2 years ago at Liqui...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Etiquette- bringing your own design? So I’ve b...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Art recommendations in the style of Gustave Do...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Photo Realistic Artists NYC? Hey guys, for my ...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Any recommendations for a pop culture/gaming a...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Weekly r/Tattoos Question/FreeTalk Thread! - A...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Artist/studio recommendation Brussel/Antwerp H...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Unemployment for tattoo artists I know most of...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>how to get a design? been looking to get my fi...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>How do I choose the right artist? I’m getting ...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content subreddit\n",
       "0   COMMUNITY ANNOUNCEMENT In solidarity with the ...   tattoos\n",
       "1   Weekly r/Tattoos Question/FreeTalk Thread! - A...   tattoos\n",
       "2   Enter Shikari and Architects album artwork and...   tattoos\n",
       "3   David Bowie Portrait - Healed, Done in April 2...   tattoos\n",
       "4   Photo realism artist chicago As the title sugg...   tattoos\n",
       "5   Can you do colorful Japanese/Yakuza tattoos on...   tattoos\n",
       "6   Tattoo Commission Question. Seperate Artist an...   tattoos\n",
       "7   Weekly r/Tattoos Question/FreeTalk Thread! - A...   tattoos\n",
       "8   Partial cover up / adding to a design with a d...   tattoos\n",
       "9   Lately I realized, that very famous people, ha...   tattoos\n",
       "10  Sacramento area artists? Hello, so after the q...   tattoos\n",
       "11  ISO Canadian Fish / Wildlife Artist around the...   tattoos\n",
       "12  As a client, what should I provide my artist w...   tattoos\n",
       "13  Sleeve thoughts Hello fellow redditors! I have...   tattoos\n",
       "14  Weekly r/Tattoos Question/FreeTalk Thread! - A...   tattoos\n",
       "15  Healed tattoo I got about 2 years ago at Liqui...   tattoos\n",
       "16  Etiquette- bringing your own design? So I’ve b...   tattoos\n",
       "17  Art recommendations in the style of Gustave Do...   tattoos\n",
       "18  Photo Realistic Artists NYC? Hey guys, for my ...   tattoos\n",
       "19  Any recommendations for a pop culture/gaming a...   tattoos\n",
       "20  Weekly r/Tattoos Question/FreeTalk Thread! - A...   tattoos\n",
       "21  Artist/studio recommendation Brussel/Antwerp H...   tattoos\n",
       "22  Unemployment for tattoo artists I know most of...   tattoos\n",
       "23  how to get a design? been looking to get my fi...   tattoos\n",
       "24  How do I choose the right artist? I’m getting ...   tattoos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize df\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data with this function\n",
    "def cleaning_fn(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    4. Returns in lowercase.\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    clean = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    clean = ''.join(clean)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in clean.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the rows of the df so we don't have the iloc 1-100 all classified\n",
    "# as one class, the next 200 as another, etc., so we don't have issues with\n",
    "# a train/test split\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I am Dr. Buzz Aldrin, back again on reddit. I ...</td>\n",
       "      <td>IAmA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In pottery class I made a container to hold fl...</td>\n",
       "      <td>Jokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>What happens if one star in a binary pair goes...</td>\n",
       "      <td>askscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>My girlfriend told me to take the spider out i...</td>\n",
       "      <td>Jokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Randomly wanting to break up with bf? Hey, so ...</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Imagine having a job at a Candy Store That wou...</td>\n",
       "      <td>dadjokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>I was reading the history of the French Revolu...</td>\n",
       "      <td>dadjokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ELI5: why is it that when I'm boiling pasta wi...</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Did you know shower heads are bisexual Every n...</td>\n",
       "      <td>Jokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>PSA: The \"Fab Wallpapers\" app which is ranked ...</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Wells Fargo opened another 2 accounts in my na...</td>\n",
       "      <td>personalfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>[PART 1] The Orphanage in the Woods: The Apple...</td>\n",
       "      <td>nosleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>upgrading me PC i currently have a i3-7100 and...</td>\n",
       "      <td>buildapc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>My (33f) bf (33m) keeps condescendingly saying...</td>\n",
       "      <td>relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>FYI - A \"pilot episode\" is a specific thing, n...</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>I think I deleted file explorer and I can't fi...</td>\n",
       "      <td>buildapc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>ELI5: what mechanism allows seed to \"hibernate...</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>LPT: in every new city you visit, allow yourse...</td>\n",
       "      <td>LifeProTips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Change my opinion: MongoDB 4.2 over RDBMS So, ...</td>\n",
       "      <td>webdev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Can I afford this house? Looking at buying a h...</td>\n",
       "      <td>personalfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Season 7 of Game Of Thrones is like a tabletop...</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No credit card, but somehow have a decent cred...</td>\n",
       "      <td>personalfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Thank you This isn't a dad joke. This is a tha...</td>\n",
       "      <td>dadjokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>20 players have averaged 30pts/5reb/10ast... ....</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>[D] Churn Prediction Model In Production affec...</td>\n",
       "      <td>MachineLearning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content          subreddit\n",
       "0   I am Dr. Buzz Aldrin, back again on reddit. I ...               IAmA\n",
       "1   In pottery class I made a container to hold fl...              Jokes\n",
       "2   What happens if one star in a binary pair goes...         askscience\n",
       "3   My girlfriend told me to take the spider out i...              Jokes\n",
       "4   Randomly wanting to break up with bf? Hey, so ...    TwoXChromosomes\n",
       "5   Imagine having a job at a Candy Store That wou...           dadjokes\n",
       "6   I was reading the history of the French Revolu...           dadjokes\n",
       "7   ELI5: why is it that when I'm boiling pasta wi...  explainlikeimfive\n",
       "8   Did you know shower heads are bisexual Every n...              Jokes\n",
       "9   PSA: The \"Fab Wallpapers\" app which is ranked ...            Android\n",
       "10  Wells Fargo opened another 2 accounts in my na...    personalfinance\n",
       "11  [PART 1] The Orphanage in the Woods: The Apple...            nosleep\n",
       "12  upgrading me PC i currently have a i3-7100 and...           buildapc\n",
       "13  My (33f) bf (33m) keeps condescendingly saying...      relationships\n",
       "14  FYI - A \"pilot episode\" is a specific thing, n...         television\n",
       "15  I think I deleted file explorer and I can't fi...           buildapc\n",
       "16  ELI5: what mechanism allows seed to \"hibernate...  explainlikeimfive\n",
       "17  LPT: in every new city you visit, allow yourse...        LifeProTips\n",
       "18  Change my opinion: MongoDB 4.2 over RDBMS So, ...             webdev\n",
       "19  Can I afford this house? Looking at buying a h...    personalfinance\n",
       "20  Season 7 of Game Of Thrones is like a tabletop...         television\n",
       "21  No credit card, but somehow have a decent cred...    personalfinance\n",
       "22  Thank you This isn't a dad joke. This is a tha...           dadjokes\n",
       "23  20 players have averaged 30pts/5reb/10ast... ....                nba\n",
       "24  [D] Churn Prediction Model In Production affec...    MachineLearning"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the df['subreddit'] is no longer grouped by class but it sorted at random.\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The actual predictive model with three examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25652,)\n",
      "(25652,)\n",
      "(6413,)\n",
      "(6413,)\n"
     ]
    }
   ],
   "source": [
    "# Apply train/test split\n",
    "X = df['content']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', RandomForestClassifier()),  # Originally trained with MulinomialNB() but had low accuracy\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tcnick12\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_patt...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit X_train and y_train on the pipe\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          Android       0.39      0.39      0.39        54\n",
      "              DIY       0.09      0.05      0.06        41\n",
      "          Fitness       0.62      0.74      0.67       222\n",
      "       Futurology       0.00      0.00      0.00        29\n",
      "            Games       0.58      0.16      0.25        44\n",
      "     GetMotivated       0.67      0.31      0.42        13\n",
      "             IAmA       0.61      0.88      0.72       270\n",
      "            Jokes       0.27      0.50      0.35       344\n",
      "      LifeProTips       0.75      0.73      0.74       177\n",
      "  MachineLearning       0.67      0.72      0.69       192\n",
      "            Music       0.29      0.25      0.27        65\n",
      "        Overwatch       0.44      0.33      0.38        82\n",
      "              PS4       0.47      0.60      0.53       135\n",
      "   Showerthoughts       0.20      0.05      0.08        39\n",
      "           Tinder       0.45      0.26      0.33        19\n",
      "  TwoXChromosomes       0.38      0.36      0.37       230\n",
      "   WritingPrompts       0.86      0.47      0.61        38\n",
      "       askscience       0.39      0.39      0.39       242\n",
      "          atheism       0.69      0.51      0.59       115\n",
      "            books       0.62      0.57      0.59       154\n",
      "         buildapc       0.64      0.82      0.72       374\n",
      "         dadjokes       0.35      0.72      0.47       354\n",
      "           europe       0.00      0.00      0.00         8\n",
      "explainlikeimfive       0.91      0.95      0.93       132\n",
      "          gadgets       0.00      0.00      0.00         7\n",
      "    gameofthrones       0.85      0.69      0.76        84\n",
      "           gaming       0.37      0.14      0.20        50\n",
      "          history       0.68      0.54      0.60       282\n",
      "  leagueoflegends       0.61      0.45      0.52       207\n",
      "        lifehacks       0.00      0.00      0.00        40\n",
      "     listentothis       0.00      0.00      0.00        25\n",
      "malefashionadvice       0.86      0.33      0.48        72\n",
      "           movies       0.62      0.14      0.23        70\n",
      "              nba       0.69      0.49      0.57       135\n",
      "          nosleep       0.81      0.93      0.87       364\n",
      "     pcmasterrace       0.25      0.06      0.10       101\n",
      "  personalfinance       0.73      0.73      0.73       369\n",
      "       philosophy       1.00      0.29      0.44        21\n",
      "          pokemon       0.74      0.36      0.48        86\n",
      "         politics       1.00      0.88      0.93        16\n",
      "    relationships       0.64      0.70      0.67       276\n",
      "           soccer       0.75      0.20      0.32        15\n",
      "            space       0.33      0.03      0.05        38\n",
      "          tattoos       1.00      0.50      0.67         4\n",
      "       technology       0.50      0.12      0.20         8\n",
      "       television       0.44      0.05      0.09        80\n",
      "             tifu       0.67      0.65      0.66       294\n",
      "           travel       0.81      0.43      0.56       166\n",
      "            trees       0.36      0.05      0.09        74\n",
      "           webdev       0.71      0.38      0.49       156\n",
      "\n",
      "         accuracy                           0.57      6413\n",
      "        macro avg       0.54      0.40      0.43      6413\n",
      "     weighted avg       0.58      0.57      0.55      6413\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tcnick12\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline prediction object with X_test\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Score the model with X_test and y_test\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IAmA', 'Jokes', 'askscience', 'TwoXChromosomes', 'dadjokes',\n",
       "       'explainlikeimfive', 'Android', 'personalfinance', 'nosleep',\n",
       "       'buildapc', 'relationships', 'television', 'LifeProTips', 'webdev',\n",
       "       'nba', 'MachineLearning', 'atheism', 'history', 'tifu',\n",
       "       'WritingPrompts', 'space', 'movies', 'pokemon', 'leagueoflegends',\n",
       "       'travel', 'pcmasterrace', 'philosophy', 'malefashionadvice',\n",
       "       'Music', 'Fitness', 'GetMotivated', 'DIY', 'gameofthrones',\n",
       "       'Games', 'books', 'trees', 'lifehacks', 'Showerthoughts', 'PS4',\n",
       "       'politics', 'Tinder', 'Futurology', 'Overwatch', 'soccer',\n",
       "       'gaming', 'listentothis', 'tattoos', 'gadgets', 'europe',\n",
       "       'technology'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at these categories, try four fake reviews and see how the model does:\n",
    "df.subreddit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fn that takes in a reddit post and returns the top five most likely categories:\n",
    "def get_predictions(post, num_answers=5):\n",
    "  \"\"\" takes a post and returns the top categories it fits in \"\"\"\n",
    "\n",
    "  # get the predicted probabilities for each class\n",
    "  preds = pd.Series(pipeline.predict_proba(post)[0])\n",
    "\n",
    "  # save each class to the Series index\n",
    "  preds.index = pipeline.classes_\n",
    "\n",
    "  # sort to get the most likely classes\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  # return the top num_answers results in dict format\n",
    "  return preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test one with a fake review about history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a fake review\n",
    "history_post = [ \"\"\"\n",
    "                History if my favorite subject.  I love to read historical accounts about ancient Rome and Greece.\n",
    "                I'm also a big World War 2 buff and I collect objects with historical significance.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "history        0.8\n",
       "PS4            0.1\n",
       "LifeProTips    0.1\n",
       "webdev         0.0\n",
       "dadjokes       0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(history_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test two with a fake review about pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again and mention pokemon to see if the model correctly guesses pokemon:\n",
    "pokemon_post = [ \"\"\"\n",
    "                My favorite pokemon are pikachu and charizard.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pokemon     0.6\n",
       "dadjokes    0.3\n",
       "Jokes       0.1\n",
       "webdev      0.0\n",
       "PS4         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(pokemon_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test three with a fake post about android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a fake prediction to see if android gets predicted:\n",
    "android_post = [ \"\"\"\n",
    "                I use a galaxy note 5.  My favorite opperating system version was oreo.\n",
    "                Android phones are better than iphones. I like to create apps for the app store.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Android           0.3\n",
       "askscience        0.2\n",
       "IAmA              0.1\n",
       "dadjokes          0.1\n",
       "Showerthoughts    0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(android_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test four with a fake post about music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a fake prediction to see if music gets predicted:\n",
    "music_post = [ \"\"\"\n",
    "                I love to listen to music.  My favorite singer/songwriter is Foy Vance.  Every so often\n",
    "                I like to listen to Bob Marley.  I have a large vinyl music collection but more recently I've\n",
    "                been listening to everything on Spotify.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Music         0.3\n",
       "dadjokes      0.2\n",
       "askscience    0.2\n",
       "trees         0.1\n",
       "gaming        0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(music_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "# save the model\n",
    "dump(pipeline, open('reddit_model_nc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load in the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "# load the model\n",
    "loaded_model = load(open('reddit_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Flask app API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code meant to be in a Flask app.  Won't run on colab\n",
    "\n",
    "from pickle import load\n",
    "# load the model\n",
    "loaded_model = load(open('reddit_model.pkl', 'rb'))\n",
    "\n",
    "\n",
    "from flask import jsonify\n",
    "\n",
    "@app.route(\"/predict.json\", methods=[\"POST\"])\n",
    "def predict():\n",
    "  print(\"PREDICT ROUTE...\")\n",
    "  print(\"FORM DATA:\", dict(request.form))\n",
    "  #> {'title': 'example title', 'text': 'Example reddit post text here'}\n",
    "\n",
    "  # concatenate title and text, passed in as one variable to the model\n",
    "  post = request.form[\"title\"] + ' ' + screen_name_b = request.form[\"text\"]\n",
    "\n",
    "  # get predictions, store as a Pandas Series\n",
    "  preds = pd.Series(loaded_model.predict_proba(music_post)[0])\n",
    "\n",
    "  # assign the subreddit classes to the index\n",
    "  preds.index = loaded_model.classes_\n",
    "\n",
    "  # sort by values to get the top results\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  # return the top 5 results as JSON\n",
    "  return jsonify(subreddits=preds.index[:5],\n",
    "                  probabilities=preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
